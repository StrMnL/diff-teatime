---
title: "为何求导？为何自动求导？"
description: "简单来说，在机器学习中，优化一个模型需要对很复杂的复合函数求导，这太难了。所以机器学习框架为我们提供了自动求导的方法。但我们为什么非要求导呢？"
published: 2026-02-08
# updated: 2026-02-08
tags: [机器学习, 深度学习, 微分]
category: 笔记
series: "动手学深度学习"
draft: false
---

:::important[免责声明]
- 本篇笔记是介绍自动微分之前的过渡和引入。
- 文中包含一些个人理解。惟笔记作者水平有限，**内容难免有疏漏、模糊、不严谨、不规范之处**。
:::

有了对向量导数的基本了解和对梯度的理解，原书下一节的内容就来到了自动微分。

自动微分，即由机器学习框架自动完成求导。那么，它应该也可以完成向量导数和梯度的计算。所以在介绍完微分的内容后介绍怎么用 PyTorch 做微分，很自然吧？（很自然吗？）

不过，我们又为什么要求导呢？为什么先前要着重介绍梯度的意义呢？这其实是个很重要的问题。

## 简单来说

为什么我们需要**自动**求导？

早在[机器学习中的微分](/posts/d2l-preliminaries-calculus-1/)的引入部分就已经提到：

> 机器学习中优化问题的求解还需要用到微分的知识。

在机器学习中，优化一个模型需要对很复杂的复合函数求导，这是一个极大的挑战。所以机器学习框架为我们提供了自动求导的方法，极大地简化了求导的过程。这就是我们要介绍自动求导的理由。

## 为什么要求导

但是，为什么我们通过求导的办法来优化模型？

不如说，我们好像甚至没有解释过「优化」和「模型」到底是什么？

在机器学习与深度学习中，模型指的就是一个**多层的复合函数**。我们希望这个复合函数能够根据特定的输入，给出一个有用的输出，从而解决一些问题。

暂时忽略掉一些细节，可以这样讲：模型中每一层函数都有可调节的参数。我们可以调节这些参数来改变它接受一定的输入后的输出。我们希望通过调整参数，找到合适参数组合，让模型的输出尽可能符合我们预期，这就是模型的**训练**。

在训练过程中，我们构造出一个**损失函数**（一般来说是个标量），用来量化模型的输出与预期输出的偏差，或是参数本身的一些特征（例如之前提到过的非 0 参数数量）等与我们预期的差距。调整参数和更换样本（样本是输入和**预期**输出的组合）都会影响损失函数的取值，所以损失函数是关于**样本和参数**的函数。

不过，我们的目标是调整参数的取值，来让已知的一批样本经过模型计算后能得到更符合预期的结果，以及让参数本身的形式更符合我们的预期（这就是**优化**，至于让模型能够适应更广泛的样本，那是**泛化**的工作）。所以，我们主要关心**损失函数关于参数的变化趋势**。

所以，我们**首先只使用一组样本**。在这组样本下，损失函数只是关于参数的函数。我们可以将它对参数**求梯度**，由此显示出我们往哪个「**方向**」调整参数可以**让损失函数的值有更明显的减小**，为我们调整参数提供数学上的依据。

**这就是我们为什么需要求导**。可以说，这是我们训练模型时的一大核心任务。

## 话又说回来了

神经网络的复合结构极其复杂，可以有非常多的复合函数层级和海量的参数，手工求导并非易事。

幸运的是，PyTorch 这样的框架提供了自动求导的方法，它可以**根据确定的函数复合结构**，求一个函数在自变量取特定值时的导数，这让模型优化变得简单、可行。

至于更多的细节，例如模型应该如何构造，损失函数又该如何构造，以及我们要如何利用一整批样本完成一次完整的参数调整，这是后面才要关心的事了。