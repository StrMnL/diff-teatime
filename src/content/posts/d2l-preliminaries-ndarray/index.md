---
title: "N 维数组及其基础操作"
description: "n 维数组是处理大批量数据时常用的数据结构，也是机器学习中主要使用的数据结构。"
published: 2026-01-28
updated: 2026-02-01
tags: [机器学习, 深度学习, PyTorch, 数据结构]
category: 笔记
series: "动手学深度学习"
draft: false
---

:::note[20260201 更新内容]

- 对用索引修改数组的方式以及数组的 `reshape()` 方法做了补充；
- 增加了免责声明；
- 其它用词和排版微调。

:::

:::important[免责声明]

本文是一篇**笔记**，其中包含一些**并不直接源于**文末列出的参考文献的个人理解与经验总结。由于作者认知水平有限，文中难免存在疏漏、模糊之处。如果对部分内容存在疑问，请参考其它社区内容或权威文档。

:::

## n 维数组

我们定义单个数值为 0 维数组，数值的有序排列构成一维数组，用一对 `[]` 包裹起来。在此基础上，n 维数组在形式上就是「直接元素」是同形的 n-1 维数组的数组。多层的嵌套构成了数据结构的层级和实际意义。

|维数|名称|意义|实例|
|----|----|----|----|
|0|标量|数值|灰度值|
|1|向量|特征|RGB 颜色值|
|2|矩阵|样本|一行/一列像素点的颜色|
|3|-|-|一张图片|
|4|-|-|一批图片；一个视频|
|5|-|-|一批视频|
|...|...|...|...|

:::warning[注意]

- n 维数组是一个「数表」，它的「维数」仅仅是数据结构的层级数，或者说在空间上呈现这个「数表」需要的「维度数」（也称为轴数），不具有代数上的意义。**向量都是一维数组。**
- 同时，数组的维数也未必和实际意义对应。例如，在上面的例子中，一张图片显然是一个「二维」的对象，但它的数据结构是一个三维数组。**数组的维数是一个抽象的概念。**
- 写法上，向量都写成行向量。所以习惯上将矩阵写成每行一个向量的形式（用 `print` 打印时 PyTorch 生成的样式也是这样）。在后面会看到例子。

:::

:::tip[提示]

- n 维数组仅仅是一个数据结构，它的意义在实际情况下可以灵活变化，数组的维数并不绝对与特征、样本等概念绑定。
- **我们一般认为最内层的标量是数组对象作为一个整体所具有的「元素」**。此处除非有特殊说明或着重强调，比如使用「数组某一层的直接元素」这样的表述，否则本文会用「元素」代指数组中的标量。
- n 维数组在深度学习框架中常被称作张量（tensor），虽然它其实和数学上的张量有点区别。

:::

## 入门操作

### 创建数组

要创建一个数组，需要确定：

- 形状（shape）。如 3 行、4 列、深度为 2。
- 元素的数据类型（dtype）。如 `float32`。
- 每个元素的值。

:::warning[注意]

- 确定了形状也就隐式地确定了维数和大小（即元素个数）。
- 一个数组的大小是确定的，一旦创建元素个数不能改变。但你可以用数组的 `reshape()` 方法在不改变大小的前提来改变数组的形状，详见[形状](#形状)部分的有关说明。
- 所有的元素具有相同的数据类型。

:::

```python
import torch
arr = torch.tensor(
    [[1, 2, 3, 4],
    [5, 6, 7, 8],
    [9, 10, 11, 12]]
)
```

这里用 `torch.tensor()` 方法，通过传入一个 Python 嵌套列表来生成一个 $3\times4$ 数组 `arr`。这个数组会作为后面的例子。

`dtype` 参数是可选的，这里应该会自动使用合适的整数类型。如果需要手动指定，只需传入数据类型对应的对象即可，如 `dtype=torch.float64`。

除此之外，还有一些其它方法可以创建一个数组，例如：

- `torch.arange()` 可以创建一个向量，和 Python 的 `range()` 类似。
- `torch.zeros()` 可以创建一个指定形状的全 0 数组。关于形状的表示，详见[形状](#形状)部分。
- `torch.ones()` 可以创建一个指定形状的全 1 数组。
- `torch.randn()` 可以创建一个指定形状的随机数组，其中的元素所服从正态分布（高斯分布）。
- `torch.zeros_like()` 可以创建一个与传入的数组形状和数据类型都相同的全 0 数组。类似地也有 `torch.ones_like()` 和 `torch.randn_like()` 这样的方法。这在一些内存管理技巧中可能会使用到。详见[内存管理](#内存管理)部分。

### 访问数组中的数据

在一个 n 维数组对象后加上**中括号**，就可以用索引和切片来访问数组。

要取出单个元素，索引是「**由外到内**」逐层访问的。这也是符合 n 维数组在形式上的写法的。

例如，对于上面定义的数组 `arr`：

- 第一个数是第一层的索引。如 `arr[-1]` 可以访问倒数第一行，得到 `[9, 10, 11, 12]`。
- 第二个数是第二层的索引。如 `arr[2, 3]` 可以访问第三行、第四列，得到 `12`。

当数组的维数更高时，索引的方式类似，先访问「直接元素」，再访问直接元素的直接元素，以此类推。

:::warning[注意]

索引从 0 开始，而不是从 1 开始，这和 Python 的列表是一样的。

:::

使用切片可以得到数组的分块，每一层的切片的写法也和 Python 的列表类似。

例如，`arr[1:, :2]` 可以得到 `[[5, 6], [9, 10]]`。你还可以使用带有步长的写法，`arr[1:, :2]` 相当于 `arr[1::1, :2:1]`。

:::caution[当心]

要取出第三行前两个元素，你既可以写 `arr[2, :2]`，也可以写 `arr[2:, :2]`。但是，由于前一种写法在访问行时使用了整数索引，所以得到的是一维的 `[9, 10]`；而后一种写法得到的是二维的 `[[9, 10]]`。

:::

除此之外，还有一些更加灵活的索引，例如可以传入一个同形的布尔数组，需要的时候可以查查文档。

:::warning[注意]

- 你不仅可以用这种方式读数组，还可以用 `arr[:] = 2` 这种写法来修改数组。后面[内存管理](#内存管理)部分的例子也用了这种方法。
- 而且，这种写法也存在类似于后面提到的[广播](#广播)的机制（上面这个例子就使用了这个机制，尽管右边甚至不是一个同形的 `tensor`，而是一个 Python 整数，但它可以将 `arr` 的每个元素都改为 `2`）。详情可以查阅官方文档。
- 但是，在数组与其它对象之间共用内存的情况下，修改数组可能会带来意想不到的结果。具体可以参考[形状](#形状)部分对 `reshape()` 的补充说明，以及[类型转换](#类型转换)部分。**总之，在修改数组时需要保持谨慎。**

:::

### 形状

数组具有 `shape` 属性，表示数组的形状。它是一个 `torch.Size` 对象，你可以把它当成一个元组。

例如对上面定义的数组 `arr`，用 `arr.shape` 访问它的形状。它会返回结果 `torch.Size([3, 4])`，你可以把它当成 `(3, 4)` 处理。这表示它第一层有 3 个直接元素，第二层有 4 个直接元素，即它是一个 3 行 4 列的矩阵。

在需要表示数组形状时，例如用前面提到的用 `torch.zeros()` 方法创建数组，你可以使用一个元组。

例如，`torch.zeros((2, 3, 4))` 会创建一个这样的数组：

```python showLineNumbers=false
tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
```

第一层有 2 个直接元素，第二层有 3 个直接元素，第三层有 4 个直接元素。用 `print` 打印这个数组，PyTorch 会将它显示成两个 $3\times4$ 矩阵的形式。

:::tip[提示]

`torch.zeros(2, 3, 4)` 这样的写法也是允许的，即直接传入多个数而不是一个元组。这能够被`zeros()` 方法正确处理。而且，因为剩余的参数都是关键字参数，这并不会有什么影响。`ones()` 和 `randn()` 以及后面的介绍的 `reshape()` 方法也支持这种写法。

:::

:::warning[注意]

`torch.zeros((1, 3, 4))` 会生成一个三维数组，尽管第一层只有一个直接元素；`torch.zeros((3, 4))` 则生成一个二维数组。不过，它们的大小是相同的，可以通过下面的 `reshape()` 方法互化。

:::

如果你希望在不改变大小和数组元素的值的情况下改变数组的形状，你可以使用数组的 `reshape()` 方法。

例如对前面的 `arr` 数组：

```python
arr_new = arr.reshape(4,3)
```

你会得到这样的 `arr_new`：

```python showLineNumbers=false
tensor([[ 1,  2,  3],
        [ 4,  5,  6],
        [ 7,  8,  9],
        [10, 11, 12]])
```

:::warning[注意]

- 形式上， `reshape()` 方法不改变数组本身，而是返回一个新的数组。但是，有时它并不真正「复制」原数组并创建一个新数组，详见下面「当心」部分的说明。
- 在能够确定新形状时（即只缺一个参数），可用 `-1` 代替某一轴上的长度，让 `reshape()` 自动处理。

:::

:::caution[当心]

- 对连续内存上的数组，`reshape()` 方法返回的是对原数组的一个引用，返回的数组与原数组仍指向同一块内存，与后面[类型转换](#类型转换)部分提到的 `torch.tensor` 和 `numpy.ndarray` 之间的转换类似。如果你尝试更改返回后的数组，原数组也会发生变化。
- 如果数组并不在连续的内存上，则返回的数组不会与原数组共用内存（而是分配一块新的内存）。

:::


另外，数组还具有 `numel()` 方法（不是属性哦），可以返回数组的大小，即形状中各轴上的长度之积，也即数组中元素的个数（`numel` 是 number of elements 的缩写）。

例如，`arr.numel()` 会返回 `12`。

## 按元素运算

数组支持按元素（elementwise）的运算，即对每个位置上的元素独立地做运算。为了理解这一点，可以参考下面的例子。

### 一元运算

数组可以做一元运算，例如 `torch.exp(x)` 可以对每个元素取以 $e$ 为底数的幂。

注意这里会返回新数组而不改变原数组。

### 二元运算

对于两个同形的数组，做二元运算会对数组中每一对位置对应的元素做相应的运算。

例如：

```python
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y
```

得到：

```python showLineNumbers=false
tensor([ 3.,  4.,  6., 10.]),  # x + y
tensor([-1.,  0.,  2.,  6.]),  # x - y
tensor([ 2.,  4.,  8., 16.]),  # x * y
tensor([0.5000, 1.0000, 2.0000, 4.0000]),  # x / y
tensor([ 1.,  4., 16., 64.])  # x ** y
```

:::warning[注意]
`torch.tensor([1.0, 2, 4, 8])` 中，由于 `1.0` 是浮点数而其他数是整数，所以其它元素也会被转换为浮点数类型（这里自动使用了 `float32`）。`x` 和 `y` 运算时，结果也会被转换为浮点数。
:::

:::tip[提示]
虽然 PyTorch 支持向量内积运算，也支持二阶数组之间的矩阵乘法，不过这并不是默认的行为。进行这些运算的方法会在「线性代数」部分介绍。不过同形数组的加减行为和同形矩阵之间的加减是一致的。
:::

数组还可以做逻辑运算，得到一个布尔数组。例如：

```python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
X == Y
```

会得到：

```python showLineNumbers=false
tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])
```

大小比较也是可以的，如 `X >= 5` 会将每个元素与 `5` 比较，同样返回一个布尔数组。

### 求和

还可以用数组的 `sum()` 方法对各元素求和，返回一个标量。例如 `X.sum()` 会得到：

```python showLineNumbers=false
tensor(66.)
```

:::warning[注意]
返回值时标量，而不是 Python 浮点数或整数。如果需要转换为 Python 浮点数或整数，方法见[类型转换](#类型转换)。
:::

## 连结

可以用 `torch.cat()` 方法把多个数组连结（concatenate）在一起，返回一个更大的数组。例如：

```python
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```

得到两个新数组：

```python showLineNumbers=false
(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]),
 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
```

这里第一个数组沿第 0 个轴，也就是在第一层连结，所以 `Y` 的向量被添加到 `X` 的向量后；第二个数组沿第 1 个轴，也就是在第二层中连结，此时会对第一层每个向量依次做连结，所以 `Y` 每一行的元素被添加到 `X` 的对应行后面。

## 一些机制

### 广播

有时，对于一些不同形的数组，仍然可以进行按元素的运算。PyTorch 会使用广播机制（broadcasting mechanism）来处理。

> 这种机制的工作方式如下：
> 1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个新数组具有相同的形状；
> 2. 对生成的新数组执行按元素操作。

例如：

```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
```

得到：
```python showLineNumbers=false
(tensor([[0],
         [1],
         [2]]),
 tensor([[0, 1]]))
```
一般而言，广播机制会作用于长度为 1 的轴。

这里，`a` 和 `b` 分别是 $3\times1$ 和 $1\times2$ 矩阵，它们不同形。如果尝试做运算 `a + b`，`a` 和 `b` 会分别被广播为这样两个 $3\times2$ 数组：

```python showLineNumbers=false
(tensor([[0, 0],
         [1, 1],
         [2, 2]]),
 tensor([[0, 1],
         [0, 1],
         [0, 1]]))
```

然后再做按元素运算，得到：

```python showLineNumbers=false
tensor([[0, 1],
        [1, 2],
        [2, 3]])
```

:::warning[注意]

如果做运算 `a * b`，同样会通过广播机制处理，而不是做矩阵乘法。

广播也可以手动进行，具体可以查询文档。

:::

### 内存管理

由于在实际场景中，需要处理的数组可能非常大，因此需要注意内存管理的问题。

有些操作可能会为新结果重新分配内存，而不是直接在已分配的内存上覆盖掉原来的数组，即使在表达式上它「覆盖」了原来的数组。

例如，对于 `Y = X + Y`，Python 会先计算 `X + Y`，然后为计算结果重新分配一块内存，最后将 `Y` 指向新分配的内存。下面这个例子可以验证这一点，它应该会返回 `False`。

```python
before = id(Y)
Y = Y + X
id(Y) == before
```

当数组的内存占用非常大时，重新分配内存会带来不必要的开销。

在上面的例子中，在 `Y` 被指向新地址前， `Y` 先前占用的内存不会被释放。也就是说，在运算的中间过程，会有多一份和 `X`、`Y` 大小相当的内存被占用。

与此同时，重新分配内存也会为处理器带来额外的工作，尤其在内存占用极大时，这会导致程序运行的耗时增加。

而且，由于某些原因，原有的内存可能仍然在别处被引用，导致运算结束后，它不仅没有被正确释放，其值还会在某处被错误地使用（你可能更希望使用新的 `Y`）。

如果你可以静态地确定新数组的形状和数据类型，你可以使用一些技巧来限制内存的分配行为。

这里 `X` 和 `Y` 是 `dtype` 都为 `float32` 的同形数组，它们相加的结果仍然是一个具有同样形式的数组，即占用相同大小的内存，那么为什么不直接用结果覆盖掉已有的数组呢（如果 `X` 或 `Y` 后面不需要再使用）？

通过用索引 `[:]` 改变数组（或者使用 `+=`）可以做到这件事。例如，可以这样将 `X + Y` 的结果存入 `X` 中：

```python
before = id(X)
X[:] = X + Y  # equals X += Y
id(X) == before
```

运算前后 `X` 仍然占用同一块内存。

或者，有时你会希望手动预先划分一块空间用于储存结果（只要你可以预先确定结果的形状与数据类型），这可以通过创建一个全 0 数组，然后用索引的方式为它赋值来做到。例如，如果你要对上面的 `X + Y` 运算这么做：

```python
Z = torch.zeros_like(Y)
Z[:] = X + Y
```

前面提到过的 `torch.zeros_like()` 方法可以在这里用上。这样就不用手动地设定 `Z` 的形状和数据类型。

### 类型转换

PyTorch 的 `tensor` 可以与 NumPy 中的 `ndarray` 互化。

要将一个张量转换为 `ndarray`，只需使用它的 `numpy()` 方法。如 `Z.numpy()` 会返回一个相应的 `numpy.ndarray` 对象。

要将一个 `ndarray` 转换为 `tensor`，只需要将它传入 `torch.from_numpy()` 方法。

:::caution[当心]

- `tensor` 与 `ndarray` 互相转换前后的两个对象默认指向同一块内存（出于避免内存重新分配的考虑），这会导致当你操作一个对象时，另一个对象的值也发生改变。
- 但在一些情况下，转换会创建新的对象。如果 `tensor` 位于 GPU 上，用 `numpy()` 转换为 `ndarray` 会返回一个拷贝；如果使用 `torch.tensor()` 方法来将 `ndarray` 转换为 `tensor`，也会创建新的 `tensor`，而不是与原来的 `ndarray` 共享内存。

:::

除此之外，标量可以转换为 Python 内置的浮点数或整数对象。对 0 维数组使用 `item()` 方法，或者用 Python 内置的类型转换方法 `int()` 和 `float()` 即可。

---

## 参考文献

1. Zhang A, Lipton Z C, Li M, et al. Dive into Deep Learning. Cambridge University Press, 2023. [https://D2L.ai](https://D2L.ai).