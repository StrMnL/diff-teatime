---
title: "机器学习中的线性代数"
description: "机器学习中大量用到向量和矩阵的运算，线性代数是这些运算的基础。"
published: 2026-02-02
# updated: 2026-02-02
tags: [机器学习, 深度学习, PyTorch, 数学, 线性代数]
category: 笔记
series: "动手学深度学习"
draft: false
---

:::important[免责声明]
- **这是一篇笔记**，内容整理自《动手学深度学习》的 PyTorch 版本和该书的[在线课程](https://space.bilibili.com/1567748478)。
- 文中也包含一些个人理解，惟笔记作者认知水平有限，**内容难免有疏漏、模糊之处**。  
- 同时，由于文中使用的 Python 包仍在不断更新，所以本文和原书的内容都**可能会在某一天过时**。
- 总之，如果对部分内容存在疑问，请查阅原书或最新的社区内容和权威文档。
:::

## 标量

标量是一个只有大小的数值，与有方向的矢量相对。

标量可以做四则运算和函数运算。运算法则相对简单，在此略过。

标量有绝对值。且关于绝对值有以下性质：

- $|a\pm b|\leq|a|+|b|$
- $|a\cdot b|=|a|\cdot|b|$

PyTorch 中标量的创建和运算操作并没有什么特殊之处，在此不做赘述。可以参考之前的 [N 维数组及其基础操作](/posts/d2l-preliminaries-ndarray/)中的内容。下面介绍向量、矩阵、张量时也只做必要的补充。

## 向量

向量具有大小和方向。向量可以表示为有序数组，也可以表示为以坐标原点为起点的有向箭头。

向量的按元素运算[前面的笔记](/posts/d2l-preliminaries-ndarray/#按元素运算)中也已经介绍过。向量加法、标量乘法、点乘也相对基础，在此不再赘述。

另外，向量加法和同形向量的按元素加法是一致的；由于广播机制的存在，用标量或 Python 的内置数值类型乘上向量，结果也和标量乘法是一致的。

向量具有长度（模），假设 $\mathbf{a}$ 是一个 $m$ 维向量，将它的模定义为：

$$
||\mathbf{a}||_2=\sqrt{\sum_{i=1}^{m}a_i^2}
$$

:::tip[提示]
这里的写法 $||\quad||$ 表示范数。向量的模是它的 $L_2$ 范数，所以带有下标 2。对范数的介绍[见后](#范数)。习惯上会省略 $L_2$ 范数的下标，就像下面的性质一样。
:::

向量的模具有以下性质：

- 非负性：$||\mathbf{a}||\geq0$
- 齐次性：$||k \mathbf{a}|| = |k| \cdot ||\mathbf{a}||, \quad k\in\R$
- 满足三角不等式：$||\mathbf{a}+\mathbf{b}||\leq||\mathbf{a}||+||\mathbf{b}||$

习惯上，多将向量写成列向量。不过，在 n 维数组中将向量写成一行显然要方便一些。在表示表格数据集的矩阵中，会将每个样本作为矩阵中的行向量。

PyTorch 中的向量和 Python 的列表（以及字符串等「类数组」对象）一样，可以用内置的 `len()` 方法获取其长度，返回一个 Python 整数。

可以用 `torch.dot(x, y)` 方法计算两个向量 $\mathbf{x}$ 和 $\mathbf{y}$ 的点乘。实际上，点乘等价于 `torch.sum(x * y)`，即按元素乘法后求和。

## 矩阵

向量的有序排列可以写成矩阵。

矩阵的加法、数乘和按元素运算与向量类似；转置、对称矩阵与反对称矩阵等内容也比较简单，同样不详细展开。

可以用符号 $\odot$ 表示同形矩阵按元素的乘法，在数学上这称为 Hadamard 积。

这里对矩阵乘法做一点说明：

一个列向量左乘矩阵会输出一个新向量。这体现了向量空间之间的一种映射关系（更具体地来说，这种映射还满足线性性，称为**线性映射**）。矩阵是对这种映射关系的一种表示。矩阵 $\mathbf{A}$ 左乘矩阵 $\mathbf{B}$ 可以视为将 $\mathbf{A}$ 中的各个列向量依次左乘矩阵 $\mathbf{B}$，并将得到的新向量按原顺序「放入」新矩阵中。

线性映射是「对向量空间的一种扭曲」，可以表示旋转、剪切等变换。这种观点可以进一步用来解释特征向量来特征值等内容。详情可以参考 [3Blue1Brown](https://space.bilibili.com/88461692) 的系列视频《[线性代数的本质](https://www.bilibili.com/video/BV1Ys411k7yQ)》。这里的链接指向第一集。

在 PyTorch 中，可以访问矩阵的 `T` 属性得到它的转置。

:::warning[注意]
- 转置后的矩阵与原矩阵仍指向同一块内存。如果需要显式地创建一个新矩阵，可以这样写：`A.T.clone()`。
- 转置后的矩阵在逻辑上的「顺序」与内存中的顺序变得不一致，即它在内存上是「不连续的」。如果对转置后的矩阵使用 `reshape()` 方法，就会像[前面的笔记](/posts/d2l-preliminaries-ndarray/#形状)提到的那样，为结果重新分配内存。
:::

可以使用 `torch.mv(A, x)` 方法计算矩阵 $\mathbf{A}$ 和向量 $\mathbf{x}$ 的乘积（假设 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 而 $\mathbf{x} \in \mathbb{R}^{n}$）。`mv` 是 `matrix` 和 `vector` 的首字母。类似地可以用 `torch.mm()` 方法计算矩阵与矩阵的乘法。

:::warning[注意]
这里使用的是左乘。二维数组的写法和它表示的矩阵一致；一维数组虽然写成 n 维行向量，但这里是被当成列向量来计算的，而计算结果又写成行的形式，相当于 `torch.mm(A, x.reshape(n, 1)).reshape(m)`。
:::

## 张量

形式上，张量是对向量和数组构造方式的推广。n 维数组之所以可以被称为张量，是因为张量的代数结构的确就和 n 维数组的数据结构一致。

机器学习对高阶张量的要求并不那么高，能够理解这种数据结构即可。

[前面的笔记](/posts/d2l-preliminaries-ndarray/#求和)提到过张量的求和方法 `sum()`，这个方法的默认行为是对所有元素求和。不过，也可以为它指定 `dim` 参数，这表示仅沿某个或某些维度求和，并在求和后会发生「**降维**」（简单来说，相应的那一层中括号在求和后会被去掉）。

:::tip[提示]
`dim` 也可以写成 `axis`。后者是 NumPy 中的写法，PyTorch 兼容这种写法。
:::

例如用 `A = torch.arange(20, dtype=torch.float32).reshape(5, 4)` 创建一个矩阵 `A`：

```python showLineNumbers=false
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
```
`A_sum_dim0 = A.sum(dim=0)` 表示对第一层中括号内的元素，即行向量求和，可以得到这样的求和结果：

```python showLineNumbers=false
tensor([40., 45., 50., 55.])
```

而 `A_sum_dim1 = A.sum(dim=1)` 表示对每个第二层中括号内的元素分别求和：

```python showLineNumbers=false
tensor([ 6., 22., 38., 54., 70.])
```

:::tip[提示]
在第一个例子中，求和后去掉了最外面的那层中括号；而在第二个例子中，求和后第二层中括号被去掉，导致结果变成了行向量。如果你想要在第一个例子中保留最外面一层中括号，或是在第二个例子中得到一个 $5\times1$ 矩阵（列向量），可以再加上一个 `keepdims=True` 参数。
:::

除此之外，`mean()` 方法可以求均值，实际上就是求和后除以相应的元素个数。`A.mean(dim=0)` 等价于 `A.sum(dim=0) / A.shape[0]`。

`cumsum()` 方法可以沿某个 `dim` 做累积求和，例如用 `A.cumsum(dim=0)` 对 $\mathbf{A}$ 累积求和的结果如下：

```python showLineNumbers=false
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
```

`cumsum()` 方法不会发生降维。

## 范数

### 向量的范数

简单来说，范数是一个函数。它接受一个向量，然后输出一个数。它用来刻画向量的「长度」，或者说「它的分量们的大小」。

前面介绍的向量的模是典型的范数。范数都具有**非负性和齐次性，并满足三角不等式**。

以下是范数的一些类型：

- $L_0$ 范数：向量中非 0 元素的个数。$L_0$ 范数不是严格意义上的范数，因为它显然不满足齐次性。
- $L_1$ 范数：向量所有元素的**绝对值之和**。
- $L_2$ 范数：向量所有元素的平方和的平方根，即向量的模长。
- $L_p$ 范数：更一般地，$||\mathbf{x}||_p=\left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}}$。
- $L_{\infty}$ 范数：特别地，$||\mathbf{x}||_\infty=\mathop{\max\limits_i}|x_i|$，即取向量最大的分量值。

### 矩阵的范数

可以由向量推广到矩阵，定义矩阵的范数。

比较常用的矩阵范数之一是元素形式范数。元素形式范数即按元素来计算的范数，和向量范数的形式一致，所以也称为向量式范数。即：

$$
\|\mathbf{X}\|_p = (\sum_{i=1}^m \sum_{j=1}^n x_{ij}^p)^{\frac{1}{p}}
$$

特别地，$p=2$ 时的元素形式范数也被称为 $Frobenius$ 范数，简称 $F$ 范数；

当 $p=\infty$ 时，$||\mathbf{X}||_\infty=\mathop{\max\limits_{i,j}}|x_{ij}|$。

:::note[一点超纲内容]
基于范数的构造形式以及它们的一些特别的性质，可以用它来做一些事。

比如，你在训练模型时的过程中，可能希望在获得好的效果的同时，让尽量少的特征参与运算（以简化模型）。也就是说，含有更多 0 值的参数组合是更加理想的，因为参数归零，对应的特征就不再参与运算。

你可能会想到，可以将所有参数写成一个向量，然后在损失函数后加上这个向量的 $L_0$ 范数。因为 $L_0$ 范数统计的正是向量中非 0 元素的个数，所以用这样的损失函数来训练，就能够将非 0 参数的个数也纳入「损失」的考虑范围内。（不过，$L_0$ 范数在实际使用时存在一些问题。实际上一般会使用性质更好的 $L_1$ 范数来代替 $L_0$ 范数。）
:::

在 PyTorch 中，可以用 `torch.norm(u, p)` 方法求一个向量 $\mathbf{u}$ 的 $L_p$ 范数（矩阵的元素形式范数也可以使用这个方法）。和范数的写法一样，`p` 的默认值是 2。

---

## 参考文献

1. Zhang A, Lipton Z C, Li M, et al. Dive into Deep Learning. Cambridge University Press, 2023. [https://D2L.ai](https://D2L.ai).